{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Applied Machine Learning Homework 5**\n",
        "**Due 02 Dec 2023 (Tuesday) 11:59PM EST**"
      ],
      "metadata": {
        "id": "CsciWCcwSvTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions\n",
        "\n",
        "1) Please push the .ipynb and .pdf to Github Classroom prior to the deadline, .py file is optional (not needed).\n",
        "\n",
        "2) Please include your Name and UNI below."
      ],
      "metadata": {
        "id": "ES5E7BQBTnnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Name :\n",
        "\n",
        "## UNI :"
      ],
      "metadata": {
        "id": "D0SIMW0OTr9m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional Neural Network\n",
        "\n",
        "In this part of the homework, we will build and train a classical convolutional neural network on the CIFAR Dataset"
      ],
      "metadata": {
        "id": "sNOxhZujV_jK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten\n",
        "import pandas as pd\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import cifar10"
      ],
      "metadata": {
        "id": "I6NNBcucXClM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_dev, y_dev), (x_test, y_test) = cifar10.load_data()\n",
        "print(\"x_dev: {},y_dev: {},x_test: {},y_test: {}\".format(x_dev.shape, y_dev.shape, x_test.shape, y_test.shape))\n",
        "\n",
        "x_dev, x_test = x_dev.astype('float32'), x_test.astype('float32')\n",
        "x_dev = x_dev/255.0\n",
        "x_test = x_test/255.0\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(x_dev, y_dev,test_size = 0.2, random_state = 42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jd4jHLzTYuZm",
        "outputId": "5ad9cd67-4f12-49c2-daee-398ce3b5bbbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 3s 0us/step\n",
            "x_dev: (50000, 32, 32, 3),y_dev: (50000, 1),x_test: (10000, 32, 32, 3),y_test: (10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1 We will be implementing the one of the first CNN models put forward by Yann LeCunn, which is commonly refered to as LeNet-5. The network has the following layers:\n",
        "1) 2D convolutional layer with 6 filters, 5x5 kernel, stride of 1 padded to yield the same size as input, ReLU activation\n",
        "\n",
        "2) Maxpooling layer of 2x2\n",
        "\n",
        "3) 2D convolutional layer with 16 filters, 5x5 kernel, 0 padding, ReLU activation\n",
        "\n",
        "4 )Maxpooling layer of 2x2\n",
        "\n",
        "5) 2D convolutional layer with 120 filters, 5x5 kernel, ReLU activation. Note that this layer has 120 output channels (filters), and each channel has only 1 number. The output of this layer is just a vector with 120 units!\n",
        "\n",
        "6) A fully connected layer with 84 units, ReLU activation\n",
        "\n",
        "7) The output layer where each unit respresents the probability of image being in that category. What activation function should you use in this layer? (You should know this)"
      ],
      "metadata": {
        "id": "bDPnZzg0XDQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Your code here"
      ],
      "metadata": {
        "id": "ok_DyDq_XJwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2 Report the model summary"
      ],
      "metadata": {
        "id": "zKkSBBDwXNT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Your code here"
      ],
      "metadata": {
        "id": "sJlJnEcUXQY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.3 Model Training\n",
        "\n",
        "1) Train the model for 20 epochs. In each epoch, record the loss and metric (chosen in part 3) scores for both train and validation sets.\n",
        "\n",
        "2) Plot a separate plots for:\n",
        "\n",
        "* displaying train vs validation loss over each epoch\n",
        "* displaying train vs validation accuracy over each epoch\n",
        "\n",
        "3) Report the model performance on the test set. Feel free to tune the hyperparameters such as batch size and optimizers to achieve better performance."
      ],
      "metadata": {
        "id": "sjMqpYrkXXO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Your code here"
      ],
      "metadata": {
        "id": "Cyk188RpXYGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Your code here"
      ],
      "metadata": {
        "id": "2IuzTifDXece"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Your code here"
      ],
      "metadata": {
        "id": "jS4GnYarXgy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.4 Overfitting\n",
        "1) To overcome overfitting, we will train the network again with dropout this time. For hidden layers use dropout probability of 0.3. Train the model again for 20 epochs. Report model performance on test set.\n",
        "\n",
        "Plot a separate plots for:\n",
        "\n",
        "displaying train vs validation loss over each epoch\n",
        "displaying train vs validation accuracy over each epoch\n",
        "2) This time, let's apply a batch normalization after every hidden layer, train the model for 20 epochs, report model performance on test set as above.\n",
        "\n",
        "Plot a separate plots for:\n",
        "\n",
        "displaying train vs validation loss over each epoch\n",
        "displaying train vs validation accuracy over each epoch\n",
        "3) Compare batch normalization technique with the original model and with dropout, which technique do you think helps with overfitting better?"
      ],
      "metadata": {
        "id": "mSteTYz8XlFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.4.1 Dropout"
      ],
      "metadata": {
        "id": "QqC6Zs_UXrHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Your code here"
      ],
      "metadata": {
        "id": "tKi2YqTrXulb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Your code here"
      ],
      "metadata": {
        "id": "tr_FL8h5Xx5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Your code here"
      ],
      "metadata": {
        "id": "b8GRnh8dX0WF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.4.2 Batch Normalisation"
      ],
      "metadata": {
        "id": "-xviUopQX71P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Your code here"
      ],
      "metadata": {
        "id": "HgvBTLRpX-t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Your code here"
      ],
      "metadata": {
        "id": "oSdB71AQYBkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Your code here"
      ],
      "metadata": {
        "id": "Dy1glawqYExM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Any reasonable explanation for whichever method performs better."
      ],
      "metadata": {
        "id": "e7ZTCCRuYG4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Natural Language Processing\n",
        "\n",
        "We will train a supervised training model to predict if a tweet has a positive or negative sentiment."
      ],
      "metadata": {
        "id": "z0sQIvXhTzNp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Dataset loading & dev/test splits**"
      ],
      "metadata": {
        "id": "QXMFcomQT3cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1) Load the movie reviews dataset from NLTK library"
      ],
      "metadata": {
        "id": "X-AVsvHdMtqQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqyybzOMMAvo",
        "outputId": "05e45f5d-0efa-48d8-ceb7-87f8179e3a53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(\"movie_reviews\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2) Load the positive & negative reviews"
      ],
      "metadata": {
        "id": "fli0xSP5MpOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "2anjMGF6MYmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3) Make a data frame that has all reviews and their corresponding labels"
      ],
      "metadata": {
        "id": "wgnT0SFqM5WN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "iLehhhrQM4h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.4) Look at the class distribution of the tweets"
      ],
      "metadata": {
        "id": "ttE7HrHjNDuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "NtuvFJQ_NBmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.5) Create a development & test split (80/20 ratio):"
      ],
      "metadata": {
        "id": "hn05L1C0NIx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "egvHN2VeNGwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data preprocessing**\n",
        "\n",
        "We will do some data preprocessing before we tokenize the data. We will remove # symbol, hyperlinks, stop words & punctuations from the data. You can use the re package in python to and and replace these strings."
      ],
      "metadata": {
        "id": "7v8ikOSFNRUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.6) Replace numbers in every review with \"\""
      ],
      "metadata": {
        "id": "G1rs4Z0sOGN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "sJ8HffyQNPyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.7) Remove all stop words"
      ],
      "metadata": {
        "id": "GUESPUzDOO6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "Kz_qmWLkOVVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.9) Remove all punctuations"
      ],
      "metadata": {
        "id": "y5OIknvjOZwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "TK1E0DXNOTWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.10) Apply stemming on the development & test datasets using Porter algorithm"
      ],
      "metadata": {
        "id": "TgCMUOoHOdtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "y-rKDUheOa3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.11) Looking at the dataset do you think any other data pre-processing might be helpful? You can experiment with the features if you want to."
      ],
      "metadata": {
        "id": "BsgZEUY1Q7M1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Model Training"
      ],
      "metadata": {
        "id": "htyGAullliDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1) Create bag of words features for each review in the development dataset"
      ],
      "metadata": {
        "id": "UkPGh4VSltnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "mbkVmAwRluTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2) Train a Logistic Regression model on the development dataset"
      ],
      "metadata": {
        "id": "1lfwegKtk-Lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "PEx3JOnfk-tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3) Create TF-IDF features for each review in the development dataset"
      ],
      "metadata": {
        "id": "B_3wvbk9k_Sj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "vlmA7bS3k_tQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.4) Train the Logistic Regression model on the development dataset with TF-IDF features"
      ],
      "metadata": {
        "id": "uFnzO67WlL2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "mF1AcJxKlN_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.5) Compare the performance of the two models on the test dataset using a classication\n",
        "report and the scores obtained. Explain the difference in results obtained."
      ],
      "metadata": {
        "id": "MdFS6SWMlONK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "o6gpOsp7lTJx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}